<!doctype html><html lang=en><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Ronalds Vilcins - http://localhost:1313/"><title>| Vinay Kumar</title>
<meta name=description content><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="Naïve Bayes Classifiers # In this article we will talk about Multinomial Naïve Bayes Classifier, so called because it is a Bayesian Classifier that makes a simplifying (naïve) assumption about the interaaction b/w features.
Let&rsquo;s understand the intuition of this calssifier in the context of text classification. Given a text document we first respresnt the text document as a bag-of-words (i.e. an unordered set of words in the document with their position information removed) keeping only the word-frequency in the given document."><meta property="og:title" content><meta property="og:description" content="Naïve Bayes Classifiers # In this article we will talk about Multinomial Naïve Bayes Classifier, so called because it is a Bayesian Classifier that makes a simplifying (naïve) assumption about the interaaction b/w features.
Let&rsquo;s understand the intuition of this calssifier in the context of text classification. Given a text document we first respresnt the text document as a bag-of-words (i.e. an unordered set of words in the document with their position information removed) keeping only the word-frequency in the given document."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/post/nlp/naive_bayes/"><meta property="article:section" content="post"><meta itemprop=name content><meta itemprop=description content="Naïve Bayes Classifiers # In this article we will talk about Multinomial Naïve Bayes Classifier, so called because it is a Bayesian Classifier that makes a simplifying (naïve) assumption about the interaaction b/w features.
Let&rsquo;s understand the intuition of this calssifier in the context of text classification. Given a text document we first respresnt the text document as a bag-of-words (i.e. an unordered set of words in the document with their position information removed) keeping only the word-frequency in the given document."><meta itemprop=wordCount content="136"><meta itemprop=keywords content><link rel=canonical href=http://localhost:1313/post/nlp/naive_bayes/><link rel=dns-prefetch href=https://www.google-analytics.com><link href=https://www.google-analytics.com rel=preconnect crossorigin><link rel=alternate type=application/atom+xml title="Vinay Kumar" href=http://localhost:1313/atom.xml><link rel=alternate type=application/json title="Vinay Kumar" href=http://localhost:1313/feed.json><link rel="shortcut icon" type=image/png href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII="><style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeSpeed}.posts hr{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:none;border-bottom:1px #353535;flex:1 0 1rem}main{max-width:70ch;padding:2ch;margin:auto}a,body{color:#353535}::selection,a:focus,a:hover{background-color:#353535;color:#fff}.meta{margin:0 0 2.5rem}.tags::before{content:"\2022";margin-left:1rem}code,pre{color:#353535;font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;border:1px solid #353535;font-size:small}.highlight [class^=language-]{color:#fff}code{padding:.1rem;border:none}pre{padding:.5rem;overflow-x:auto}pre code{border:none}img{max-width:100%;border:1px solid #353535}hr{background:#353535;height:1px;border:0}ul{list-style-type:square}ul,ol{padding-left:1.2rem}header li,footer li{display:inline;text-transform:uppercase}header a,footer a{text-decoration:none}header ul,footer ul{justify-content:space-between;display:flex}[aria-current=page]{text-decoration:line-through}header,section,footer{padding:1rem 0}blockquote{border-left:5px solid #353535;padding-left:1rem}.posts ul,header ul,footer ul{list-style:none}.posts,header ul,footer ul{padding:0}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.7rem}.posts li a,.posts li div{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}.hash{opacity:.25;text-decoration:none}table{border-collapse:collapse;text-align:left;width:100%}table tr{background:#fff;border-bottom:1px solid}table th,table td{padding:10px 20px}</style><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"post","name":"","headline":"","alternativeHeadline":"","description":"Naïve Bayes Classifiers # In this article we will talk about Multinomial Naïve Bayes Classifier, so called because it is a Bayesian Classifier that makes a simplifying (naïve) assumption about the interaaction b\/w features.\nLet\u0026rsquo;s understand the intuition of this calssifier in the context of text classification. Given a text document we first respresnt the text document as a bag-of-words (i.e. an unordered set of words in the document with their position information removed) keeping only the word-frequency in the given document.","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/localhost:1313\/post\/nlp\/naive_bayes\/"},"author":{"@type":"Person","name":""},"creator":{"@type":"Person","name":""},"accountablePerson":{"@type":"Person","name":""},"copyrightHolder":"Vinay Kumar","copyrightYear":"0001","dateCreated":"0001-01-01T00:00:00.00Z","datePublished":"0001-01-01T00:00:00.00Z","dateModified":"0001-01-01T00:00:00.00Z","publisher":{"@type":"Organization","name":"Vinay Kumar","url":"http://localhost:1313/","logo":{"@type":"ImageObject","url":"http:\/\/localhost:1313\/","width":"32","height":"32"}},"image":"http://localhost:1313/","url":"http:\/\/localhost:1313\/post\/nlp\/naive_bayes\/","wordCount":"136","genre":[],"keywords":[]}</script></head><body><main><header><nav><ul><li><a href=/>Index</a></li><li><a href=/about/>About</a></li><li><a href=/atom.xml>RSS</a></li></ul></nav></header><hr><section><span itemprop=articleBody><h1 id=naïve-bayes-classifiers>Naïve Bayes Classifiers <a href=#na%c3%afve-bayes-classifiers class=hash>#</a></h1><p>In this article we will talk about <strong>Multinomial Naïve Bayes Classifier</strong>,
so called because it is a <em>Bayesian Classifier that makes a simplifying (naïve)
assumption about the interaaction b/w features</em>.</p><p>Let&rsquo;s understand the intuition of this calssifier in the context of <strong>text classification</strong>.
Given a text document we first respresnt the text document as a <strong>bag-of-words</strong>
(i.e. an unordered set of words in the document with their position information removed)
keeping only the <strong>word-frequency</strong> in the given document.
In this <strong>bag-of-words</strong> representation, all we care about is how many times a given word appears in this document.</p><blockquote><p><strong>Naïve Bayes</strong> is a <em>probabilistic classifier</em>, meaning that for a given document <strong><code>d</code></strong>,
out of all classes $c\in C$ the classifier returns the class $\hat{c}$ which has the maximum
posterior probability given the document <strong><code>d</code></strong>.</p></blockquote></span></section><hr><footer><nav><ul><li>© 2025</li></ul></nav></footer></main></body></html>