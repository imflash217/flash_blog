<!doctype html><html lang=en><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Ronalds Vilcins - http://localhost:1313/"><title>| Vinay Kumar</title>
<meta name=description content><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="Probablity Theory # A key concept in the field of pattern recognition is that of uncertainity. It arrises through:
noise on measurements finite size of datasets. Probability theory provides a consistent framework for the quantification and manipulation of uncertainity and forms one of the central foundations of Pattern Recognition.
When combined with Decision Theory, it allows us to make optimal predictions given all the informtion available to us, even though that information may be incomplete or ambiguous."><meta property="og:title" content><meta property="og:description" content="Probablity Theory # A key concept in the field of pattern recognition is that of uncertainity. It arrises through:
noise on measurements finite size of datasets. Probability theory provides a consistent framework for the quantification and manipulation of uncertainity and forms one of the central foundations of Pattern Recognition.
When combined with Decision Theory, it allows us to make optimal predictions given all the informtion available to us, even though that information may be incomplete or ambiguous."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/post/blogs/prob/intro/"><meta property="article:section" content="post"><meta itemprop=name content><meta itemprop=description content="Probablity Theory # A key concept in the field of pattern recognition is that of uncertainity. It arrises through:
noise on measurements finite size of datasets. Probability theory provides a consistent framework for the quantification and manipulation of uncertainity and forms one of the central foundations of Pattern Recognition.
When combined with Decision Theory, it allows us to make optimal predictions given all the informtion available to us, even though that information may be incomplete or ambiguous."><meta itemprop=wordCount content="370"><meta itemprop=keywords content><link rel=canonical href=http://localhost:1313/post/blogs/prob/intro/><link rel=dns-prefetch href=https://www.google-analytics.com><link href=https://www.google-analytics.com rel=preconnect crossorigin><link rel=alternate type=application/atom+xml title="Vinay Kumar" href=http://localhost:1313/atom.xml><link rel=alternate type=application/json title="Vinay Kumar" href=http://localhost:1313/feed.json><link rel="shortcut icon" type=image/png href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII="><style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeSpeed}.posts hr{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:none;border-bottom:1px #353535;flex:1 0 1rem}main{max-width:70ch;padding:2ch;margin:auto}a,body{color:#353535}::selection,a:focus,a:hover{background-color:#353535;color:#fff}.meta{margin:0 0 2.5rem}.tags::before{content:"\2022";margin-left:1rem}code,pre{color:#353535;font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;border:1px solid #353535;font-size:small}.highlight [class^=language-]{color:#fff}code{padding:.1rem;border:none}pre{padding:.5rem;overflow-x:auto}pre code{border:none}img{max-width:100%;border:1px solid #353535}hr{background:#353535;height:1px;border:0}ul{list-style-type:square}ul,ol{padding-left:1.2rem}header li,footer li{display:inline;text-transform:uppercase}header a,footer a{text-decoration:none}header ul,footer ul{justify-content:space-between;display:flex}[aria-current=page]{text-decoration:line-through}header,section,footer{padding:1rem 0}blockquote{border-left:5px solid #353535;padding-left:1rem}.posts ul,header ul,footer ul{list-style:none}.posts,header ul,footer ul{padding:0}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.7rem}.posts li a,.posts li div{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}.hash{opacity:.25;text-decoration:none}table{border-collapse:collapse;text-align:left;width:100%}table tr{background:#fff;border-bottom:1px solid}table th,table td{padding:10px 20px}</style><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"post","name":"","headline":"","alternativeHeadline":"","description":"Probablity Theory # A key concept in the field of pattern recognition is that of uncertainity. It arrises through:\nnoise on measurements finite size of datasets. Probability theory provides a consistent framework for the quantification and manipulation of uncertainity and forms one of the central foundations of Pattern Recognition.\nWhen combined with Decision Theory, it allows us to make optimal predictions given all the informtion available to us, even though that information may be incomplete or ambiguous.","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/localhost:1313\/post\/blogs\/prob\/intro\/"},"author":{"@type":"Person","name":""},"creator":{"@type":"Person","name":""},"accountablePerson":{"@type":"Person","name":""},"copyrightHolder":"Vinay Kumar","copyrightYear":"0001","dateCreated":"0001-01-01T00:00:00.00Z","datePublished":"0001-01-01T00:00:00.00Z","dateModified":"0001-01-01T00:00:00.00Z","publisher":{"@type":"Organization","name":"Vinay Kumar","url":"http://localhost:1313/","logo":{"@type":"ImageObject","url":"http:\/\/localhost:1313\/","width":"32","height":"32"}},"image":"http://localhost:1313/","url":"http:\/\/localhost:1313\/post\/blogs\/prob\/intro\/","wordCount":"370","genre":[],"keywords":[]}</script></head><body><main><header><nav><ul><li><a href=/>Index</a></li><li><a href=/about/>About</a></li><li><a href=/atom.xml>RSS</a></li></ul></nav></header><hr><section><span itemprop=articleBody><h1 id=probablity-theory>Probablity Theory <a href=#probablity-theory class=hash>#</a></h1><p>A key concept in the field of <strong>pattern recognition</strong> is that of <strong>uncertainity</strong>.
It arrises through:</p><ol><li>noise on measurements</li><li>finite size of datasets.</li></ol><p>Probability theory provides a consistent framework for the quantification
and manipulation of uncertainity and forms one of the central foundations
of Pattern Recognition.</p><p>When combined with <strong>Decision Theory</strong>, it allows us to make optimal predictions
given all the informtion available to us, even though that information may be incomplete or ambiguous.</p><h2 id=example-redblue-boxes>Example: Red/Blue boxes <a href=#example-redblue-boxes class=hash>#</a></h2><h3 id=the-setup>The setup <a href=#the-setup class=hash>#</a></h3><p>Imagine we have two boxes <code>red_box</code> and <code>blue_box</code>.
Each box contains different number of fruits <code>oranges</code> and <code>apples</code>.</p><pre><code>red_box:
    - apples: 2
    - oranges: 6
blue_box:
    - apples: 3
    - oranges: 1
</code></pre><h3 id=the-experiment>The Experiment <a href=#the-experiment class=hash>#</a></h3><p>Now, we do the following steps:</p><ol><li><em>Select a box</em>: <strong>Randomly</strong> pick one of the boxes (either <code>red_box</code> or <code>blue_box</code>)</li><li><em>Pick a fruit</em>: Then, <strong>randomly</strong> select an item of fruit for the box.</li><li><em>Replace the fruit</em>: Having observed the type of the picked fruit (<code>apple</code> or <code>orange</code>),
now, replace it in the box from which it came.</li><li><em>Repeat</em> steps 1-to-3 multiple times.</li></ol><h3 id=the-pre-conditions--assumptions>The Pre-conditions / Assumptions <a href=#the-pre-conditions--assumptions class=hash>#</a></h3><p>Now, let&rsquo;s suppose in doing the above experiment,</p><ol><li>we pick the <code>red_box</code> <strong>40%</strong> of the times and the <code>blue_box</code> <strong>60%</strong> of the times.</li><li>Also, assume that that when we remove an item of fruit
from a box, we are equally likely to select any of the pices of fruits in the box.</li></ol><h3 id=the-theory>The Theory <a href=#the-theory class=hash>#</a></h3><p>In this experiemnt:</p><ol><li>the identity of the box to be chosen is a <strong>random variable</strong> $B$ which can take
two possible values <code>r</code> and <code>b</code> (corresponding to red or blue boxes).</li><li>Similarly, the identity of fruit is also a <strong>random variable</strong> $F$ and it can
take either of the values <code>a</code> and <code>o</code> (corresponding to apple an dorange respectively).</li></ol><blockquote><p>Definition:</p><p>The <strong>Probability of an event</strong> is defined as the <em>fraction</em> of times, that event occurs
out of the total number of trials (in the limit that the total number of trials goes to
infinity). All probabilities must lie in the interval [0, 1]</p></blockquote><p>Thus , the probability of selecting the <code>red_box</code> is $4/10$ and that of the <code>blue_box</code> is
$6/10$.</p><p>$$
p(B=r) = 4/10
$$</p><p>$$
p(B=b) = 6/10
$$</p></span></section><hr><footer><nav><ul><li>Â© 2025</li></ul></nav></footer></main></body></html>