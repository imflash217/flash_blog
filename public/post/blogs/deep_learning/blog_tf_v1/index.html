<!doctype html><html lang=en><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Ronalds Vilcins - http://localhost:1313/"><title>| Vinay Kumar</title>
<meta name=description content><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content='Tensorflow Tutorial # In this session you will learn to do the following in TensorFlow v1.0
Initialize Variables Start your own session Train Algorithms Implement a Neural Network Exploring the Tensorflow Library # Example-1: General Overview # import tensorflow as tf y_hat = tf.constant(36, name="y_hat") ## Defins a "y_hat" constant. Sets its value to 36 y = tf.constant(39, name="y") ## Defins a "y" constant. Sets its value to 39 loss = tf.'><meta property="og:title" content><meta property="og:description" content='Tensorflow Tutorial # In this session you will learn to do the following in TensorFlow v1.0
Initialize Variables Start your own session Train Algorithms Implement a Neural Network Exploring the Tensorflow Library # Example-1: General Overview # import tensorflow as tf y_hat = tf.constant(36, name="y_hat") ## Defins a "y_hat" constant. Sets its value to 36 y = tf.constant(39, name="y") ## Defins a "y" constant. Sets its value to 39 loss = tf.'><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/post/blogs/deep_learning/blog_tf_v1/"><meta property="article:section" content="post"><meta itemprop=name content><meta itemprop=description content='Tensorflow Tutorial # In this session you will learn to do the following in TensorFlow v1.0
Initialize Variables Start your own session Train Algorithms Implement a Neural Network Exploring the Tensorflow Library # Example-1: General Overview # import tensorflow as tf y_hat = tf.constant(36, name="y_hat") ## Defins a "y_hat" constant. Sets its value to 36 y = tf.constant(39, name="y") ## Defins a "y" constant. Sets its value to 39 loss = tf.'><meta itemprop=wordCount content="1006"><meta itemprop=keywords content><link rel=canonical href=http://localhost:1313/post/blogs/deep_learning/blog_tf_v1/><link rel=dns-prefetch href=https://www.google-analytics.com><link href=https://www.google-analytics.com rel=preconnect crossorigin><link rel=alternate type=application/atom+xml title="Vinay Kumar" href=http://localhost:1313/atom.xml><link rel=alternate type=application/json title="Vinay Kumar" href=http://localhost:1313/feed.json><link rel="shortcut icon" type=image/png href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII="><style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeSpeed}.posts hr{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:none;border-bottom:1px #353535;flex:1 0 1rem}main{max-width:70ch;padding:2ch;margin:auto}a,body{color:#353535}::selection,a:focus,a:hover{background-color:#353535;color:#fff}.meta{margin:0 0 2.5rem}.tags::before{content:"\2022";margin-left:1rem}code,pre{color:#353535;font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;border:1px solid #353535;font-size:small}.highlight [class^=language-]{color:#fff}code{padding:.1rem;border:none}pre{padding:.5rem;overflow-x:auto}pre code{border:none}img{max-width:100%;border:1px solid #353535}hr{background:#353535;height:1px;border:0}ul{list-style-type:square}ul,ol{padding-left:1.2rem}header li,footer li{display:inline;text-transform:uppercase}header a,footer a{text-decoration:none}header ul,footer ul{justify-content:space-between;display:flex}[aria-current=page]{text-decoration:line-through}header,section,footer{padding:1rem 0}blockquote{border-left:5px solid #353535;padding-left:1rem}.posts ul,header ul,footer ul{list-style:none}.posts,header ul,footer ul{padding:0}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.7rem}.posts li a,.posts li div{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}.hash{opacity:.25;text-decoration:none}table{border-collapse:collapse;text-align:left;width:100%}table tr{background:#fff;border-bottom:1px solid}table th,table td{padding:10px 20px}</style><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"post","name":"","headline":"","alternativeHeadline":"","description":"Tensorflow Tutorial # In this session you will learn to do the following in TensorFlow v1.0\nInitialize Variables Start your own session Train Algorithms Implement a Neural Network Exploring the Tensorflow Library # Example-1: General Overview # import tensorflow as tf y_hat = tf.constant(36, name=\u0026quot;y_hat\u0026quot;) ## Defins a \u0026quot;y_hat\u0026quot; constant. Sets its value to 36 y = tf.constant(39, name=\u0026quot;y\u0026quot;) ## Defins a \u0026quot;y\u0026quot; constant. Sets its value to 39 loss = tf.","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/localhost:1313\/post\/blogs\/deep_learning\/blog_tf_v1\/"},"author":{"@type":"Person","name":""},"creator":{"@type":"Person","name":""},"accountablePerson":{"@type":"Person","name":""},"copyrightHolder":"Vinay Kumar","copyrightYear":"0001","dateCreated":"0001-01-01T00:00:00.00Z","datePublished":"0001-01-01T00:00:00.00Z","dateModified":"0001-01-01T00:00:00.00Z","publisher":{"@type":"Organization","name":"Vinay Kumar","url":"http://localhost:1313/","logo":{"@type":"ImageObject","url":"http:\/\/localhost:1313\/","width":"32","height":"32"}},"image":"http://localhost:1313/","url":"http:\/\/localhost:1313\/post\/blogs\/deep_learning\/blog_tf_v1\/","wordCount":"1006","genre":[],"keywords":[]}</script></head><body><main><header><nav><ul><li><a href=/>Index</a></li><li><a href=/about/>About</a></li><li><a href=/atom.xml>RSS</a></li></ul></nav></header><hr><section><span itemprop=articleBody><h1 id=tensorflow-tutorial>Tensorflow Tutorial <a href=#tensorflow-tutorial class=hash>#</a></h1><p>In this session you will learn to do the following in <code>TensorFlow v1.0</code></p><ol><li>Initialize Variables</li><li>Start your own session</li><li>Train Algorithms</li><li>Implement a Neural Network</li></ol><h2 id=exploring-the-tensorflow-library>Exploring the Tensorflow Library <a href=#exploring-the-tensorflow-library class=hash>#</a></h2><h3 id=example-1-general-overview>Example-1: General Overview <a href=#example-1-general-overview class=hash>#</a></h3><pre><code class=language-python>import tensorflow as tf

y_hat = tf.constant(36, name=&quot;y_hat&quot;)           ## Defins a &quot;y_hat&quot; constant. Sets its value to 36
y = tf.constant(39, name=&quot;y&quot;)                   ## Defins a &quot;y&quot; constant. Sets its value to 39
loss = tf.Variable((y-y_hat)**2, name=&quot;loss&quot;)

init = tf.global_variables_initializer()        ## Used to initialize the variables with the
                                                ## respective values when &quot;sess.run(init)&quot; is called

with tf.Session() as sess:                      ## Creates a session to execute our program
    sess.run(init)                              ## initializes the global variables
    sess.run(loss)                              ## executes the program stored in &quot;loss&quot; variable
    print(loss)                                 ## prints the value stored in &quot;loss&quot; variable
</code></pre><p>Writing and running programs in <code>Tensorflow</code> has the following steps:</p><ol><li><strong>Create tensors</strong> (variables) that are not yet evaluated/executed.</li><li><strong>Write operations</strong> between those tensors.</li><li><strong>Initialize</strong> the tensors.</li><li><strong>Create a Session</strong>.</li><li><strong>Run the session</strong>. This will run the operations written in step-2.</li></ol><p>So, when we created a variable for <code>loss</code>, we simply defined the loss as a function of other
quantities but did not evaluate its value. To evaluate it, we had to run
<code>tf.global_variables_initializer()</code> to intialize the values and then inside <code>sess.run(init)</code>
we calculated the updated value and prited it in the last line above.</p><h3 id=example-2-tfsession>Example-2: <code>tf.Session()</code> <a href=#example-2-tfsession class=hash>#</a></h3><p>Now, let&rsquo;s take a look at</p><pre><code class=language-python>a = tf.constant(2)
b = tf.constant(10)
c = tf.multiply(a, b)
print(c)
</code></pre><pre><code>Tensor(&quot;Mul:0&quot;, shape=(), dtype=int32)
</code></pre><p>As expected we will not see <code>20</code>. We got a tensor saying that the result of the tensor
does not have the <code>shape</code> attribute and is of the type <code>int32</code>. All we did was to put in
the <strong>computation graph</strong>; but we haven&rsquo;t run this computation yet! In order to actually
multiply the two numbers we have to create a sessiona nd run it.</p><pre><code class=language-python>sess = tf.Session()
print(sess.run(c))
</code></pre><pre><code>20
</code></pre><p><strong>Awesome!!</strong>. To summarize, remember the following:</p><ol><li>Initialize your variables.</li><li>Create a session.</li><li>Run the operations inside the session.</li></ol><h3 id=example-3-tfplaceholder>Example-3: <code>tf.placeholder()</code> <a href=#example-3-tfplaceholder class=hash>#</a></h3><p>Next, we will see how to use a placeholder.</p><p>A <strong>placeholder</strong> is an object whose value we can specify ONLY later.</p><p>To specify values for a placeholder, we can pass in values by using a
&ldquo;feed dictionary&rdquo; (<code>feed_dict</code> variable).</p><pre><code class=language-python>## Below we create a placeholder for x.
## This allows us to pass in a number later when we run the SESSION

sess = tf.Session()

x = tf.placeholder(tf.int64, name=&quot;x&quot;)      ## the placeholder variable
print(sess.run(2*x, feed_dict={x:9}))

sess.close()
</code></pre><pre><code>18
</code></pre><h3 id=using-one-hot-encodings>Using one-hot encodings: <a href=#using-one-hot-encodings class=hash>#</a></h3><pre><code class=language-python>def one_hot_matrix(labels, num_classes):
    &quot;&quot;&quot;
    Creates a matrix where the i-th row corresponds to the ith class number.
    j-th column corresponds to the j-th example.
    So, if the label for j-th example is i; then only the ith value is 1 in j-th column
    
    Args:
        labels: the labels for each example
        num_classes: the number of classes in this task
    Returns:
        a one-hot matrix
    &quot;&quot;&quot;
    ## create a tf.constant &amp; name it &quot;num_classes&quot;
    num_classes = tf.constant(num_classes, name=&quot;num_classes&quot;)
    
    ## Use tf.one_hot (be careful with &quot;axis&quot;)
    one_hot_matrix = tf.one_hot(indices=labels, depth=num_classes, axis=0)

    ## Create a session
    sess = tf.Session()

    ## Execute the one_hot_matrix graph inside the session
    one_hot = sess.run(one_hot_matrix)

    ## Close the session
    sess.close()
    
    ## return the one_hot matrix
    return one_hot

</code></pre><pre><code class=language-python>import numpy as np

labels = np.array([1,2,0,1,2,2,3])
num_classes = 4
one_hot = one_hot_matrix(labels, num_classes)
print(one_hot)
</code></pre><pre><code>[[0,0,1,0,0,0,0],
 [1,0,0,1,0,0,0],
 [0,1,0,0,1,1,0],
 [0,0,0,0,0,0,1]]
</code></pre><h3 id=initialize-with-zeros--ones>Initialize with zeros & ones <a href=#initialize-with-zeros--ones class=hash>#</a></h3><p>We will use <code>tf.ones()</code> and <code>tf.zeros()</code> to initialize a tensor of shape <code>shape</code>,
where all elements are either zeros or ones</p><pre><code class=language-python>
def ones(shape, dtype=tf.int64):
    &quot;&quot;&quot;Creates a tensor of ones with shape=shape
    Args:
        shape: the shape of the resulting tensor
        dtype: the datatype of every element in the resulting tensor
    Returns:
        A tensor where all elements are 1
    &quot;&quot;&quot;
    ## Create ones tensor using `tf.ones()`
    ones = tf.ones(shape, dtype=dtype)

    ## Create a session
    sess = tf.Session()
    
    ## Execute the op in the session to calculate its value
    ones = sess.run(ones)

    ## Close the session
    sess.close()

    ## Return the ones tensor
    return ones
</code></pre><pre><code class=language-python>ones_tensor = ones([2,3])
print(ones_tensor)
</code></pre><pre><code>[[1,1,1],
 [1,1,1]]
</code></pre><h2 id=building-a-neural-network>Building a Neural Network <a href=#building-a-neural-network class=hash>#</a></h2><h3 id=building-the-model>Building the model <a href=#building-the-model class=hash>#</a></h3><pre><code class=language-python>from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict
from tensorflow.python.framework import ops

def model(X_train, Y_train, X_test, Y_test, 
        lr=1e-3, num_epochs=1500, bs=32, verbose=True):
        &quot;&quot;&quot;
        Implements a 3-layer Tensorflow Neural Network:
        [Linear]-&gt;[Relu]-&gt;[Linear]-&gt;[Relu]-&gt;[Linear]-[Softmax]
          
        Args:
            X_train: the train dataset inputs
            Y_train: the train dataset labels
            X_test: the test dataset inputs
            Y_test: the test dataset labels
            lr: the learnign rate
            num_epochs: number of epochs
            bs: batch-size
            verbose: True if you want to print the process else False
        
        Returns:
            the trained model parameters.
        &quot;&quot;&quot;
        
        ops.reset_default_graph()       ## to be able to rerun the model, w/o overwriting the tf.variables
        tf.set_random_seed(217)         ## to keep consistent results
        seed = 3                        ## to keep consistent results
        (n_x, m) = X_train.shape        ## n_x = input size; m = number of training examples
        n_y = Y_train.shape[0]          ## n_y = output size
        costs = []                      ## to keep track of the costs

        ## Step-1: Create placeholders of shape = (n_x, n_y)
        X, Y = create_placeholders(n_x, n_y)
        
        ## Step-2: Initialize parameters
        parameters = initialize_parameters()

        ## Step-3: Forward propagation
        ##         Build the forward propagation the tf graph
        Z3 = forward_proagation(X, parameters)

        ## Step-4: Cost function
        ##         Add cost function to tf graph
        cost = compute_cost(Z3, Y)

        ## Step-5: Backward propagation
        ##         Define the tf optimizer. Use `AdamOptimizer`
        optimizer = tf.train.AdamOptimizer(lr).minimize(cost)
        
        ## Step-6: Initialize all variables
        init = tf.global_variables_initializer()

        ## Step-7: Start the session to compute the tf graph
        with tf.Session() as sess:
            ## Step-7.1: Run the initializer `init`
            sess.run(init)

            ## Step-7.2: Do the training loop
            for epoch in range(num_epchs):
                epoch_cost = 0.0        ## Define the cost for each epoch
                num_batches = m // bs
                seed += 1
                minibatches = random_mini_batches(X_train, Y_train, bs, seed)
                for (Xb, Yb) in minibatches:
                    _, minibatch_cost = sess.run([optimizer, cost], feed_dict={X:Xb, Y:Yb})
                    epoch_cost += minibatch_cost
                epoch_cost /= num_batches

            ## Step-8: Save the trained model parameters
            parameters = sess.run(parameters)
            print(&quot;parameters have been trained&quot;)

            ## Step-9: How to calculate the correct predictions &amp; accuracy
            correct_preds = tf.equal(tf.argmax(Z3), tf.argmax(Y))
            accuracy = tf.reduce_mean(tf.cast(correct_preds, &quot;float&quot;))

            ## Step-10: Calculate the train &amp; test accuracies
            accuracy_train = accuracy.eval({X:X_train, Y:Y_train})
            accuracy_test = accuracy.eval({X:X_test, Y:Y_test})

            return parameters
</code></pre></span></section><hr><footer><nav><ul><li>Â© 2025</li></ul></nav></footer></main></body></html>