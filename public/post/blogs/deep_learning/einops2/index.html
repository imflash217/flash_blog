<!doctype html><html lang=en><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Ronalds Vilcins - http://localhost:1313/"><title>| Vinay Kumar</title>
<meta name=description content><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="Popular Deep Learning Architectures using EINOPS # In this section we will be rewriting the building blocks of deep learning in both the traditional PyTorch way as well as using einops library.
Imports # Firstly, we will import the necessary libraries to be used.
## importing necessary libraries import math import numpy as np import torch import torch.nn as nn import torch.nn.functional as F from einops import rearrange, reduce, repeat, asnumpy, parse_shape from einops."><meta property="og:title" content><meta property="og:description" content="Popular Deep Learning Architectures using EINOPS # In this section we will be rewriting the building blocks of deep learning in both the traditional PyTorch way as well as using einops library.
Imports # Firstly, we will import the necessary libraries to be used.
## importing necessary libraries import math import numpy as np import torch import torch.nn as nn import torch.nn.functional as F from einops import rearrange, reduce, repeat, asnumpy, parse_shape from einops."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/post/blogs/deep_learning/einops2/"><meta property="article:section" content="post"><meta itemprop=name content><meta itemprop=description content="Popular Deep Learning Architectures using EINOPS # In this section we will be rewriting the building blocks of deep learning in both the traditional PyTorch way as well as using einops library.
Imports # Firstly, we will import the necessary libraries to be used.
## importing necessary libraries import math import numpy as np import torch import torch.nn as nn import torch.nn.functional as F from einops import rearrange, reduce, repeat, asnumpy, parse_shape from einops."><meta itemprop=wordCount content="3015"><meta itemprop=keywords content><link rel=canonical href=http://localhost:1313/post/blogs/deep_learning/einops2/><link rel=dns-prefetch href=https://www.google-analytics.com><link href=https://www.google-analytics.com rel=preconnect crossorigin><link rel=alternate type=application/atom+xml title="Vinay Kumar" href=http://localhost:1313/atom.xml><link rel=alternate type=application/json title="Vinay Kumar" href=http://localhost:1313/feed.json><link rel="shortcut icon" type=image/png href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII="><style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeSpeed}.posts hr{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:none;border-bottom:1px #353535;flex:1 0 1rem}main{max-width:70ch;padding:2ch;margin:auto}a,body{color:#353535}::selection,a:focus,a:hover{background-color:#353535;color:#fff}.meta{margin:0 0 2.5rem}.tags::before{content:"\2022";margin-left:1rem}code,pre{color:#353535;font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;border:1px solid #353535;font-size:small}.highlight [class^=language-]{color:#fff}code{padding:.1rem;border:none}pre{padding:.5rem;overflow-x:auto}pre code{border:none}img{max-width:100%;border:1px solid #353535}hr{background:#353535;height:1px;border:0}ul{list-style-type:square}ul,ol{padding-left:1.2rem}header li,footer li{display:inline;text-transform:uppercase}header a,footer a{text-decoration:none}header ul,footer ul{justify-content:space-between;display:flex}[aria-current=page]{text-decoration:line-through}header,section,footer{padding:1rem 0}blockquote{border-left:5px solid #353535;padding-left:1rem}.posts ul,header ul,footer ul{list-style:none}.posts,header ul,footer ul{padding:0}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.7rem}.posts li a,.posts li div{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}.hash{opacity:.25;text-decoration:none}table{border-collapse:collapse;text-align:left;width:100%}table tr{background:#fff;border-bottom:1px solid}table th,table td{padding:10px 20px}</style><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"post","name":"","headline":"","alternativeHeadline":"","description":"Popular Deep Learning Architectures using EINOPS # In this section we will be rewriting the building blocks of deep learning in both the traditional PyTorch way as well as using einops library.\nImports # Firstly, we will import the necessary libraries to be used.\n## importing necessary libraries import math import numpy as np import torch import torch.nn as nn import torch.nn.functional as F from einops import rearrange, reduce, repeat, asnumpy, parse_shape from einops.","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/localhost:1313\/post\/blogs\/deep_learning\/einops2\/"},"author":{"@type":"Person","name":""},"creator":{"@type":"Person","name":""},"accountablePerson":{"@type":"Person","name":""},"copyrightHolder":"Vinay Kumar","copyrightYear":"0001","dateCreated":"0001-01-01T00:00:00.00Z","datePublished":"0001-01-01T00:00:00.00Z","dateModified":"0001-01-01T00:00:00.00Z","publisher":{"@type":"Organization","name":"Vinay Kumar","url":"http://localhost:1313/","logo":{"@type":"ImageObject","url":"http:\/\/localhost:1313\/","width":"32","height":"32"}},"image":"http://localhost:1313/","url":"http:\/\/localhost:1313\/post\/blogs\/deep_learning\/einops2\/","wordCount":"3015","genre":[],"keywords":[]}</script></head><body><main><header><nav><ul><li><a href=/>Index</a></li><li><a href=/about/>About</a></li><li><a href=/atom.xml>RSS</a></li></ul></nav></header><hr><section><span itemprop=articleBody><h1 id=popular-deep-learning-architectures-using-einops>Popular Deep Learning Architectures using EINOPS <a href=#popular-deep-learning-architectures-using-einops class=hash>#</a></h1><p>In this section we will be rewriting the building blocks of deep learning
in both the traditional <code>PyTorch</code> way as well as using <code>einops</code> library.</p><h2 id=imports>Imports <a href=#imports class=hash>#</a></h2><p>Firstly, we will import the necessary libraries to be used.</p><pre><code class=language-python>## importing necessary libraries

import math
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

from einops import rearrange, reduce, repeat, asnumpy, parse_shape
from einops.layers.torch import Rearrange, Reduce
</code></pre><h2 id=simple-convnet>Simple ConvNet <a href=#simple-convnet class=hash>#</a></h2><p>???+ danger &ldquo;Using only PyTorch&rdquo;
Here is an implementation of a simple <strong>ConvNet</strong> using only <strong><code>PyTorch</code></strong>
without <code>einops</code>.</p><pre><code>```python hl_lines=&quot;29&quot;
class ConvNet(nn.Module):
    def __init__(self):
        super(ConvNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(20, 10)
        
    def forward(self, x):
        x = self.conv1(x)
        x = F.max_pool2d(x, 2)
        x = F.relu(x)
        
        x = self.conv2(x)
        x = self.conv2_drop(x)
        x = F.max_pool2d(x, 2)
        x = F.relu(x)

        x = x.view(-1, 320)
        x = self.fc1(x)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

## Instantiating the ConvNet class
   
conv_net_old = ConvNet()
```
</code></pre><p>???+ done &ldquo;Using EINOPS + PyTorch&rdquo;
Implementing the same above ConvNet using <strong><code>einops</code></strong> & <code>PyTorch</code></p><pre><code>```python hl_lines=&quot;9&quot;
conv_net_new = nn.Sequential(
    nn.Conv2d(1, 10, kernel_size=5),
    nn.MaxPool2d(kernel_size=2),
    nn.ReLU(),
    nn.Conv2d(10, 20, kernel_size=5),
    nn.MaxPool2d(kernel_size=2),
    nn.ReLU(),
    nn.Dropout2d(),
    Rearrange(&quot;b c h w -&gt; b (c h w)&quot;),
    nn.Linear(320, 50),
    nn.ReLU(),
    nn.Dropout(),
    nn.Linear(50, 10),
    nn.LogSoftmax(dim=1)
)
```
</code></pre><p>???+ quote &ldquo;Why prefer EINOPS implementation?&rdquo;
Following are the reasons to prefer the new implementation:</p><pre><code>- [x] In the original code, if the input is changed and the **`batch_size`** 
        is divisible by 16 (which usually is), we will get something senseless after reshaping.
    - [ ] :rotating_light: The new code using **`einops`** explicitly raise ERROR in the above scenario. Hence better!!
- [x] We won't forget to use the flag **`self.training`** with the new implementation.
- [x] Code is straightforward to read and analyze.
- [x] **`nn.Sequential`** makes printing/saving/passing trivial. 
        And there is no need in your code to **load** the model (which also has lots of benefits).
- [x] Don't need **`logsoftmax`**? Now, you can use **`conv_net_new[-1]`**. 
        Another reason to prefer **`nn.Sequential`**
- [x] ... And we culd also add **inplace `ReLU`**
</code></pre><h2 id=super-resolution>Super-resolution <a href=#super-resolution class=hash>#</a></h2><p>???+ danger &ldquo;Only PyTorch&rdquo;
```python hl_lines=&ldquo;9-10 16&rdquo;
class SuperResolutionNetOLD(nn.Module):
def <strong>init</strong>(self, upscale_factor):
super(SuperResolutionNetOLD, self).<strong>init</strong>()</p><pre><code>        self.relu = nn.ReLU()
        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))
        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))
        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))
        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))
        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.relu(self.conv3(x))
        x = self.pixel_shuffle(self.conv4(x))
        return x
```
</code></pre><p>???+ done &ldquo;Using EINOPS&rdquo;
<code>python hl_lines="10" def SuperResolutionNetNEW(upscale_factor): return nn.Sequential( nn.Conv2d(1, 64, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(64, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(32, upscale_factor ** 2, kernel_size=3, padding=1), Rearrange("b (h2 w2) h w -> b (h h2) (w w2)", h2=upscale_factor, w2=upscale_factor) )</code></p><p>???+ quote &ldquo;Improvements over the old implementation&rdquo;
- [x] No need in special instruction <strong><code>pixel_shuffle</code></strong> (& the result is transferrable b/w the frameworks)
- [x] Output does not contain a fake axis (& we could do the same for the input)
- [x] inplace <strong><code>ReLU</code></strong> used now. For high resolution images this becomes critical
and saves a lot of memory.
- [x] and all the benefits of <strong><code>nn.Sequential</code></strong></p><h2 id=gram-matrix--style-transfer>Gram Matrix / Style Transfer <a href=#gram-matrix--style-transfer class=hash>#</a></h2><p>Restyling Graam Matrix for style transfer.</p><p>???+ danger &ldquo;Original Code using ONLY PyTorch&rdquo;
The original code is already very good. First line shows what kind of input is expected.</p><pre><code>```python
def gram_matrix_old(y):
    (b, c, h, w) = y.size()
    features = y.view(b, c, h * w)
    features_t = features.transpose(1, 2)
    gram = features.bmm(features_t) / (c * h * w)
    return gram
```
</code></pre><p>???+ done &ldquo;Using EINSUM&rdquo;
<code>python hl_lines="3" def gram_matrix_new(y): b, c, h, w = y.shape return torch.einsum("bchw, bdhw -> bcd", [y, y]) / (h * w)</code></p><p>???+ quote &ldquo;Improvements&rdquo;
<strong><code>einsum</code></strong> operations should be read like:</p><pre><code>- [x] For each batch &amp; each pair of channels we sum over **`h`** and **`w`**.
- [x] The normalization is also changed, because that's how **Gram Matrix** is defined.
        Else we should call it **Normalized Gram Matrix** or alike.
</code></pre><h2 id=recurrent-models-rnns>Recurrent Models (RNNs) <a href=#recurrent-models-rnns class=hash>#</a></h2><p>???+ danger &ldquo;ONLY PyTorch&rdquo;
```python hl_lines=&ldquo;14-15&rdquo;
class RNNModelOLD(nn.Module):
&ldquo;&ldquo;&ldquo;Container module with an ENCODER, a RECURRENT module & a DECODER module&rdquo;&rdquo;&rdquo;
def <strong>init</strong>(self, ntoken, ninp, nhid, nlayers, dropout=0.5):
super(RNNModelOLD, self).<strong>init</strong>()
self.drop = nn.Dropout(dropout)
self.encoder = nn.Embedding(ntoken, ninp)
self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)
self.decoder = nn.Linear(nhid, ntoken)</p><pre><code>    def forward(self, input, hidden):
        emb = self.drop(self.encoder(input))
        output, hidden = self.rnn(emb, hidden)
        output = self.drop(output)
        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))
        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden

```
</code></pre><p>???+ done &ldquo;Using EINOPS&rdquo;
```python hl_lines=&ldquo;11 14-15&rdquo;
def RNNModelNEW(nn.Module):
&ldquo;&ldquo;&ldquo;Container module with an ENCODER, RNN & a DECODER modules.&rdquo;&rdquo;&rdquo;
def <strong>init</strong>(self, ntoken, ninp, nhid, nlayers, dropout=0.5):
super(RNNModelNEW, self).<strong>init</strong>()
self.drop = nn.Dropout(dropout)
self.encoder = nn.Embedding(ntoken, ninp)
self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)
self.decoder = nn.Linear(nhid, ntoken)</p><pre><code>    def forward(self, input, hidden):
        t, b = input.shape[:2]
        emb = self.drop(self.encoder(input))
        output, hidden = self.rnn(emb, hidden)
        output = rearrange(self.drop(output), &quot;t b nhid -&gt; (t b) nhid&quot;)
        decoded = rearrange(self.decoder(output), &quot;(t b) token -&gt; t b token&quot;, t=t, b=b)
        return decoded, hidden
```
</code></pre><h2 id=improving-rnn>Improving RNN <a href=#improving-rnn class=hash>#</a></h2><p>???+ danger &ldquo;Only PyTorch&rdquo;
```python
class RNNold(nn.Module):
def <strong>init</strong>(
self,
vocab_size,
embedding_dim,
hidden_dim,
output_dim,
n_layers,
bidirectional,
dropout
):
super().<strong>init</strong>()
self.embedding = nn.Embedding(vocab_size, embedding_dim)
self.rnn = nn.LSTM(embedding_dim,
hidden_dim,
num_layers=n_layers,
bidirectional=bidirectional,
dropout=dropout)
self.fc = nn.Linear(hidden_dim * 2, output_dim)
self.dropout = nn.Dropout(dropout)</p><pre><code>    def forward(self, x):
        ## x = [sent_len, batch_size]
        embedded = self.dropout(self.embedding(x))      ## size = [sent_len, batch_size, emb_dim]
        output, (hidden, cell) = self.rnn(embedded)
        
        ## output.shape = [sent_len, batch_size, hid_dim * num_directions]
        ## hidden.shape = [num_layers * num_directions, batch_size, hid_dim]
        ## cell.shape = [num_layers * num_directions, batch_size, hid_dim]
        
        ## concat the final dropout (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers
        ## and apply dropout
        ## hidden.size = [batch_size, hid_dim * num_directions]
        hidden = self.dropout(torch.cat([hidden[-2,:,:], hidden[-1,:,:]], dim=1))

        return self.fc(hidden.squeeze(0))
```
</code></pre><p>???+ done &ldquo;Using EINOPS&rdquo;
```python
class RNNnew(nn.Module):
def <strong>init</strong>(
self,
vocab_size,
embedding_dim,
hidden_dim,
output_dim,
n_layers,
bidirectional,
dropout
):
super().<strong>init</strong>()
self.embedding = nn.Embedding(vocab_size, embedding_dim)
self.rnn = nn.LSTM(embedding_dim,
hidden_dim,
num_layers=n_layers,
bidirectional=bidirectional,
dropout=dropout)
self.dropout = nn.Dropout(dropout)
self.directions = 2 if bidirectional else 1
self.fc = nn.Linear(hidden_dim * self.directions, output_dim)</p><pre><code>    def forward(self, x):
        embedded = self.dropout(self.embedding(x))
        output, (hidden, cell) = self.rnn(embedded)
        hidden = rearrange(hidden, &quot;(layer dir) b c -&gt; layer b (dir c)&quot;, dir=self.directions)
        
        ## take the fina layer's hidden
        return self.fc(self.dropout(hidden[-1]))
```
</code></pre><h2 id=channel-shuffle-from-shufflenet>Channel Shuffle (from ShuffleNet) <a href=#channel-shuffle-from-shufflenet class=hash>#</a></h2><p>???+ danger &ldquo;ONLY PyTorch&rdquo;
```python
def channel_shuffle_old(x, groups):
b, c, h, w = x.data.size()
channels_per_group = c // groups</p><pre><code>    ## reshape
    x = x.view(b, groups, channels_per_group, h, w)
    
    ## transpose
    ## - contiguous() is required if transpose() is used before view()
    ##   See https://github.com/pytorch/pytorch/issues/764
    x = x.transpose(1, 2).contiguous()
    
    ## flatten
    x = x.view(b, -1, h, w)
    return x
```
</code></pre><p>???+ done &ldquo;Using EINOPS&rdquo;
<code>python def channel_shuffle_new(x, groups): return rearrange(x, "b (c1 c2) h w -> b (c2 c1) h w", c1=groups)</code></p><h2 id=shufflenet>ShuffleNet <a href=#shufflenet class=hash>#</a></h2><p>???+ danger &ldquo;ONLY PyTorch&rdquo;
```python
from collections import OrderedDict</p><pre><code>def channel_shuffle(x, groups):
    b, c, h, w = x.data.size()
    channels_per_group = c // groups
    
    ## reshape
    x = x.view(b, groups, channels_per_group, h, w)

    ## transpose
    ## - contiguous() is required if transpose() is used before view()
    x = x.transpose(1, 2).contiguous()
    
    x = x.view(b, -1, h, w)
    return x

class ShuffleUnitOLD(nn.Module):
    def __init__(self, 
                 in_channels, 
                 out_channels,
                 groups=3,
                 grouped_conv=True,
                 combine=&quot;add&quot;,
    ):
        super(ShuffleUnitOLD, self).__init__()
        
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.grouped_conv = grouped_conv
        self.combine = combine
        self.groups = groups
        self.bottleneck_channels = self.out_channels // 4
        
        ## define the type of ShuffleUnit
        if self.combine == &quot;add&quot;:
            ## shuffleUnit fig-2b
            self.depthwise_stride = 1
            self._combine_func = self._add
        elif self.combine == &quot;concat&quot;:
            ## ShuffleUnit fig-2c
            self.depthwise_stride = 2
            self._combine_func = self._concat
            
            ## ensure output of the concat has the same channels
            ## as the original input channels
            self.out_channels -= self.in_channels
        else:
            raise ValueError(f&quot;Cannot combine tensors with {self.combine}.\n&quot;
                             f&quot;Only 'add' &amp; 'concat' supported.&quot;)
        
        ## Use a 1x1 grouped or non-grouped convolution to reduce input channels
        ## to bottleneck channels, as in ResNet bottleneck module.
        ## NOTE: do not use group convolution for the first conv1x1 in stage-2
        self.first_1x1_groups = self.groups if grouped_conv else 1
        
        self.g_conv_1x1_compress = self._make_grouped_conv1x1(
            self.in_channels,
            self.bottleneck_channels,
            self.first_1x1_groups,
            batch_norm=True,
            relu=True,
        )
        
        ## 3x3 depthwise convolution followed by batch normalization
        self.depthwise_conv3x3 = conv3x3(
            self.bottleneck_channels,
            self.bottleneck_channels,
            stride=self.depthwise_stride,
            groups=self.bottleneck_channels
        )
        self.bn_after_depthwise = nn.BatchNordm2d(self.bottleneck_channels)

        ## use 1x1 grouped convolution to expand from bottleneck_channels to out_channels
        self.g_conv_conv_1x1_expand = self._make_grouped_conv1x1(
            self.bottleneck_channels,
            self.out_channels,
            self.groups,
            batch_norm=True,
            relu=False
        )

    
    @staticmethod
    def _add(x, out):
        ## residual connection
        return x + out

    @staticmethod
    def _concat(x, out):
        ## concat along channel dim
        return torch.cat((x, out), 1)

    def _make_grouped_conv1x1(
        self,
        in_channels,
        out_channels,
        groups,
        batch_norm=True,
        relu=False
    ):
        modules = OrderedDict()
        conv = conv1x1(in_channels, out_channels, groups=groups)
        modules['conv1x1'] = conv

        if batch_norm:
            modules['batch_norm'] = nn.BatchNorm2d(out_channels)
        if relu:
            modules['relu'] = nn.ReLU()
        if len(modules) &gt; 1:
            return nn.Sequential(modules)
        else:
            return conv

    def forward(self, x):
        ## save for combining later with output
        residual = x
        if self.combine == &quot;concat&quot;:
            residual = F.avg_pool2d(residual, kernel_size=3, stride=2, padding=1)
        
        out = self.g_con_1x1_compress(x)
        out = channel_shuffle(out, self.groups)
        out = self.depthwise_conv3x3(out)
        out = self.bn_after_depthwise(out)
        out = self.g_conv_1x1_expand(out)
        
        out = self._combine_func(residual, out)
        return F.relu(out)
        
```
</code></pre><p>???+ done &ldquo;Using EINOPS&rdquo;
```python
class ShuffleUnitNEW(nn.Module):
def <strong>init</strong>(
self,
in_channels,
out_channels,
groups=3,
grouped_conv=True,
combine=&ldquo;add&rdquo;
):
super().<strong>init</strong>()
first_1x1_groups = groups if grouped_conv else 1
bottleneck_channels = out_channels // 4
self.combine = combine
if combine == &ldquo;add&rdquo;:
## ShuffleUnit fig-2b
self.left = Rearrange("&mldr;->&mldr;") ## identity
depthwise_stride = 1
else:
## ShuffleUnit fig-2c
self.left = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)
depthwise_stride = 2
## ensure output of concat has the same channels as the original output channels
out_channels -= in_channels
assert out_channels > 0</p><pre><code>        self.right = nn.Sequential(
            ## use a 1x1grouped or non-grouped convolution to reduce
            ## input channels to bottleneck channels as in ResNet bottleneck module.
            conv1x1(in_channels, bottleneck_channels, groups=first_1x1_groups),
            nn.BatchNorm2d(bottleneck_channels),
            nn.ReLU(inplace=True),
            ## channel shuffle
            Rearrange(&quot;b (c1 c2) h w -&gt; b (c2 c1) h w&quot;, c1=groups),
            ## 3x3 depthwise convolution followed by BatchNorm
            conv3x3(bottleneck_channels, 
                    bottleneck_channels, 
                    stride=depthwise_stride,
                    groups=bottleneck_channels),
            nn.BatchNorm2d(bottleneck_channels),
            ## Use 1x1 grouped convolution to expand from bottleneck_channels to output_channels
            conv1x1(bottleneck_channels, out_channels, groups=groups),
            nn.BatchNorm2d(out_channels),
        )

    def forward(self, x):
        if self.combine == &quot;add&quot;:
            combined = self.left(x) + self.right(x)
        else:
            combined = torch.cat([self.left(x), self.right(x)], dim=1)
        return F.relu(combined, inplace=True)
        
```
</code></pre><p>???+ quote &ldquo;Improvements&rdquo;
Rewriting the code helped to identify the following:</p><pre><code>- [x] There is no sense in doing reshuffling and not using groups in the first convolution
        (indeed in the paper it is not so). **However , the result is an equivalent model**.
- [x] It is strage that the first convolution may not be grouped,
        while the last convolution is always grouped. (**and th's different from the paper**)

Also,

- [x] There is an identity layer for pyTorch introduced here.
- [x] The last thing to do is to get rid of **`conv1x1`** and **`conv3x3`** 
        (those are not better than the standard implementation)
</code></pre><h2 id=resnet>ResNet <a href=#resnet class=hash>#</a></h2><p>???+ danger &ldquo;ONLY PyTorch&rdquo;
```python
class ResNetOLD(nn.Module):
def <strong>init</strong>(self, block, layers, num_classes=1000):
self.inplanes = 64
super(ResNetOLD, self).<strong>init</strong>()
self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
self.bn1 = nn.BatchNorm2d(64)
self.relu = nn.ReLU(inplace=True)
self.maxpool = nn.MaaxPool2d(kernel_size=3, stride=2, paadding=1)
self.layer1 = self._make_layer(block, 64, layers[0])
self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
self.avgpool = nn.AvgPool2d(7, stride=1)
self.fc = nn.Linear(512 * block.expansion, num_classes)</p><pre><code>        for m in self.modules:
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2./n))
            elif isinstance(m, nn.BatchNord2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
    
    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes*block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion),
            )
        
        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))
        
        return nn.Sequential(*layers)
        
    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        return x

```
</code></pre><p>???+ done &ldquo;Using EINOPS&rdquo;
```python
def make_layer(inplanes, planes, block, n_blocks, stride=1):
downsample = None
if stride != 1 or inplanes != planes * block.expansion:
## output-size won&rsquo;t match input-size; so adjust the residual
downsample = nn.Sequential(
nn.Conv2d(inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),
nn.BatchNorm2d(planes * block.expansion),
)</p><pre><code>    return nn.Sequential(
        block(inplanes, planes, stride, downsample),
        *[block(planes * block.expansion, planes) for _ in range(1, n_blocks)]
    )

def ResNetNEW(block, layers, num_classes=1000):
    e = block.expansion
    
    resnet = nn.Sequential(
        Rearrange(&quot;b c h w -&gt; b c h w&quot;, c=3, h=224, w=224),
        nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),
        nn.BatchNorm2d(64),
        nn.ReLU(inplace=True),
        nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
        make_layer(64,      64,  block, layers[0], stride=1),
        make_layer(64 * e,  128, block, layers[1], stride=2),
        make_layer(128 * e, 256, block, layers[2], stride=2),
        make_layer(256 * e, 512, block, layers[3], stride=2),
        ## Combined AvgPool &amp; view in one single operation
        Reduce(&quot;b c h w -&gt; b c&quot;, &quot;mean&quot;),
        n.Linear(512 * e, num_classes),
    )

    ## initialization
    for m in resnet.modules():
        if isinstance(m, nn.Conv2d):
            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
            m.weight.data.normal_(0, math.sqrt(2./n))
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1.)
            m.bias.data.zero_()
    
    return resnet

```
</code></pre><h2 id=fasttext>FastText <a href=#fasttext class=hash>#</a></h2><p>???+ danger &ldquo;ONLY PyTorch&rdquo;
```python
class FastTextOLD(nn.Module):
def <strong>init</strong>(self, vocab_size, embedding_dim, output_dim):
super().<strong>init</strong>()
self.embedding = nn.Embedding(vocab_size, embedding_dim)
self.fc = nn.Linear(embedding_dim, output_dim)</p><pre><code>    def forward(self, x):
        embedded = self.embedding(x)
        embedded = embedded.permute(1, 0, 2)
        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1)
        return self.fc(pooled)
```
</code></pre><p>???+ done &ldquo;Using EINOPS&rdquo;
<code>python hl_lines="3 7" def FastTextNEW(vocab_size, embedding_dim, output_dim): return nn.Sequential( Rearrange("t b -> t b"), nn.Embedding(vocab_size, embedding_dim), Reduce("t b c -> b c", "mean"), nn.Linear(embedding_dim, output_dim), Rearrange("b c -> b c"), )</code></p><pre><code>- [x] Here, the first and last operations (highlighted) do nothing and can be removed.
        But, were added to explicitly added to show expected input and output shape
- [x] This also gives us the flexibility of changing interface by editing a single line.
        Should you need to accept inputs of shape **`(b, t)`** we just need to change 
        the line to **`Rearrange(&quot;b t -&gt; t b&quot;)`**
</code></pre><h2 id=cnns-for-text-classification>CNNs for text classification <a href=#cnns-for-text-classification class=hash>#</a></h2><p>???+ danger &ldquo;ONLY PyTorch&rdquo;
```python
class CNNold(nn.Module):
def <strong>init</strong>(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):
super().<strong>init</strong>()
self.embedding = nn.Embedding(vocab_size, embedding_dim)
self.conv_0 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[0], embedding_dim))
self.conv_1 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[1], embedding_dim))
self.conv_2 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[2], embedding_dim))
self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)
self.dropout = nn.Dropout(dropout)</p><pre><code>    def forward(self, x):
        x = x.permute(1, 0)
        embedded = self.embedding(x)
        embedded = embedded.unsqueeze(dim=1)

        conved_0 = F.relu(self.conv_0(embedded).squeeze(dim=3))
        conved_1 = F.relu(self.conv_1(embedded).squeeze(dim=3))
        conved_2 = F.relu(self.conv_2(embedded).squeeze(dim=3))

        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(dim=2)
        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(dim=2)
        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(dim=2)
        
        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))
        return self.fc(cat)

```
</code></pre><p>???+ done &ldquo;Using EINOPS&rdquo;
```python hl_lines=&ldquo;5-6 11&rdquo;
class CNNnew(nn.Module):
def <strong>init</strong>(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):
super().<strong>init</strong>()
self.embedding = nn.Embedding(vocab_size, embedding_dim)
self.convs = nn.ModuleList([nn.Conv1d(embedding_dim, n_filters, kernel_size=size)
for size in filter_sizes])
self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)
self.dropout = nn.Dropout(dropout)</p><pre><code>    def forward(self, x):
        x = rearrange(x, &quot;t b -&gt; t b&quot;)
        emb = rearrange(self.embedding(x), &quot;t b c -&gt; b c t&quot;)
        pooled = [reduce(conv(emb), &quot;b c t -&gt; b c&quot;) for conv in self.convs]
        concatenated = rearrange(pooled, &quot;filter b c -&gt; b (filter c)&quot;)
        return self.fc(self.dropout(F.relu(concatenated)))
```
</code></pre><p>???+ quote &ldquo;Discussion&rdquo;</p><pre><code>- [x] Original code misuses **`nn.Conv2d`** while **`nn.Conv1d`** is the right choice.
- [x] New code can work with any number of **`filter_sizes`** and won't fail.
- [x] First line in the new code does nothing, but was just added for simplicity &amp; clarity of shapes.
</code></pre><h2 id=highway-convolutions>Highway Convolutions <a href=#highway-convolutions class=hash>#</a></h2><p>???+ danger &ldquo;ONLY PyTorch&rdquo;
<code>python hl_lines="4" class HighwayConv1dOLD(nn.Conv1d): def forward(self, inputs): L = super(HIghwayCon1dOLD, self).forward(inputs) H1, H2 = torch.chunk(L, 2, dim=1) ## chunk at the feature dimension torch.sigmoid_(H1) return H1 * H2 + (1.0 - H1) * inputs</code></p><p>???+ done &ldquo;Using EINOPS&rdquo;
<code>python hl_lines="4" class HighwayConv1dNEW(nn.Conv1d): def forward(self, inputs): L = super().forward(inputs) H1, H2 = rearrange(L, "b (split c) t -> split b c t", split=2) torch.sigmoid_(H1) return H1 * H2 + (1.0 - H1) * inputs</code></p><h2 id=simple-attention>Simple Attention <a href=#simple-attention class=hash>#</a></h2><p>???+ danger &ldquo;ONLY PyTorch&rdquo;
```python hl_lines=&ldquo;6 8&rdquo;
class Attention(nn.Module):
def <strong>init</strong>(self):
super(Attention, self).<strong>init</strong>()</p><pre><code>    def forward(self, K, Q, V):
        A = torch.bmm(K.transpose((1, 2), Q) / np.sqrt(Q.shape[1])
        A = F.softmax(A, dim=1)
        R = torch.bmm(V, A)
        return torch.cat((R, Q), dim=1)
```
</code></pre><p>???+ done &ldquo;Using EINOPS&rdquo;
<code>python hl_lines="3 5" def attention(K, Q, V): _, n_channels, _ = K.shape A = torch.einsum("bct,bcl->btl", [K, Q]) A = F.softmax(A * n_channels ** (-0.5), dim=1) R = torch.einsum("bct,btl->bcl", [V, A]) return torch.cat((R, Q), dim=1)</code></p><h2 id=multi-head-attention>Multi-head Attention <a href=#multi-head-attention class=hash>#</a></h2><p>???+ danger &ldquo;ONLY PyTorch&rdquo;
```python
class ScaledDotProductAttention(nn.Module):
&ldquo;&ldquo;&ldquo;Scaled Dot Product Attention&rdquo;&rdquo;&rdquo;
def <strong>init</strong>(self, temperature, attn_dropout=0.1):
super().<strong>init</strong>()
self.temperature = temperature
self.dropout = nn.Dropout(attn_dropout)
self.softmax = nn.Softmax(dim=2)</p><pre><code>    def forward(self, q, k, v, mask=None):
        attn = torch.bmm(q, k.transpose(1, 2))
        attn /= self.temperature
        if mask is not None:
            attn = attn.masked_fill(mask, -np.inf)
        attn = self.softmax(attn)
        attn = self.dropout(attn)
        output = torch.bmm(attn, v)
        return output, attn

class MultiHeadAttentionOLD(nn.Module):
    &quot;&quot;&quot;Multi Head Attention Module&quot;&quot;&quot;
    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):
        super().__init__()
        self.d_k = d_k
        self.d_v = d_v
        self.n_head = n_head
        
        self.w_qs = nn.Linear(d_model, n_head * d_k)
        self.w_ks = nn.Linear(d_model, n_head * d_k)
        self.w_vs = nn.Linear(d_model, n_head * d_v)
        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))
        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))
        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_v)))
        
        self.attention = ScaledDotProductAttention(temperature=d_k**0.5)
        self.layer_norm = nn.LayerNorm(d_model)
        self.fc = nn.Linear(n_head * d_v, d_model)
        nn.init.xavier_normal_(self.fc.weight)
        self.dropout = nn.Dropout(dropout)

    def forward(self, q, k, v, mask=None):
        d_k = self.d_k
        d_v = self.d_v
        n_head = self.n_head
        
        sz_b, len_q, _ = q.size()
        sz_b, len_k, _ = k.size()
        sz_b, len_v, _ = v.size()

        residual = q
        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)
        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)
        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)

        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k)     ## (n*b, len_q, d_k)
        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, d_k)     ## (n*b, len_k, d_k)
        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, d_v)     ## (n*b, len_v, d_v)
        
        mask = mask.repeat(n_head, 1, 1)    ## (n*b, ...)
        output, attn = self.attention(q, k, v, mask=mask)
        output = output.view(n_head, sz_b, len_q, d_v)
        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1)      ## (b, len_q, n*d_v)
        output = self.dropout(self.fc(output))
        output = self.layer_norm(output + residual)

        return output, attn
        
```
</code></pre><p>???+ done &ldquo;Using EINOPS&rdquo;
```python
class MultiHeadAttentionNEW(nn.Module):
def <strong>init</strong>(self, n_heads, d_model, d_k, d_v, dropout=0.1):
super().<strong>init</strong>()
self.n_heads = n_heads</p><pre><code>        self.w_qs = nn.Linear(d_model, n_heads * d_k)
        self.w_ks = nn.Linear(d_model, n_heads * d_k)
        self.w_vs = nn.Linear(d_model, n_heads * d_v)
        
        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d-model + d_k)))
        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))
        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_v)))

        self.fc = nn.Linear(n_heads * d_v, d_model)
        nn.init.xavier_normal_(self.fc.weight)
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model)

    def forward(self, q, k, v, mask=None):
        residual = q
        
```
</code></pre><h2 id=references>References <a href=#references class=hash>#</a></h2></span></section><hr><footer><nav><ul><li>© 2025</li></ul></nav></footer></main></body></html>