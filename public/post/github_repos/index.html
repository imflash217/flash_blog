<!doctype html><html lang=en><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Ronalds Vilcins - http://localhost:1313/"><title>| Vinay Kumar</title>
<meta name=description content><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="Name: Vinay Kumar GitHub: https://github.com/imflash217/
Notable Repositories with original implementation of AI systems:
1. Cybertron # GitHub link: https://github.com/imflash217/cybertron/blob/main/cybertron/
Summary: It implements various Transformer based algorithms from scratch in raw-python, for auto-regressive tasks, such as:
various embedding layers like: Rotary_Embedding, Absolute_Positional, Fixed_Positional, Relative_PositionBias, Alibi_PositionBias Residual & GRU gates various Normalization layers like ReZero, ScaleNorm, RMSNorm etc. A unified attention layer class to support various configurations of transformer-based architectures. Vision Transformer (ViT) Wrapper Native implementation of Encoder & Decoder blocks."><meta property="og:title" content><meta property="og:description" content="Name: Vinay Kumar GitHub: https://github.com/imflash217/
Notable Repositories with original implementation of AI systems:
1. Cybertron # GitHub link: https://github.com/imflash217/cybertron/blob/main/cybertron/
Summary: It implements various Transformer based algorithms from scratch in raw-python, for auto-regressive tasks, such as:
various embedding layers like: Rotary_Embedding, Absolute_Positional, Fixed_Positional, Relative_PositionBias, Alibi_PositionBias Residual & GRU gates various Normalization layers like ReZero, ScaleNorm, RMSNorm etc. A unified attention layer class to support various configurations of transformer-based architectures. Vision Transformer (ViT) Wrapper Native implementation of Encoder & Decoder blocks."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/post/github_repos/"><meta property="article:section" content="post"><meta itemprop=name content><meta itemprop=description content="Name: Vinay Kumar GitHub: https://github.com/imflash217/
Notable Repositories with original implementation of AI systems:
1. Cybertron # GitHub link: https://github.com/imflash217/cybertron/blob/main/cybertron/
Summary: It implements various Transformer based algorithms from scratch in raw-python, for auto-regressive tasks, such as:
various embedding layers like: Rotary_Embedding, Absolute_Positional, Fixed_Positional, Relative_PositionBias, Alibi_PositionBias Residual & GRU gates various Normalization layers like ReZero, ScaleNorm, RMSNorm etc. A unified attention layer class to support various configurations of transformer-based architectures. Vision Transformer (ViT) Wrapper Native implementation of Encoder & Decoder blocks."><meta itemprop=wordCount content="188"><meta itemprop=keywords content><link rel=canonical href=http://localhost:1313/post/github_repos/><link rel=dns-prefetch href=https://www.google-analytics.com><link href=https://www.google-analytics.com rel=preconnect crossorigin><link rel=alternate type=application/atom+xml title="Vinay Kumar" href=http://localhost:1313/atom.xml><link rel=alternate type=application/json title="Vinay Kumar" href=http://localhost:1313/feed.json><link rel="shortcut icon" type=image/png href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII="><style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeSpeed}.posts hr{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:none;border-bottom:1px #353535;flex:1 0 1rem}main{max-width:70ch;padding:2ch;margin:auto}a,body{color:#353535}::selection,a:focus,a:hover{background-color:#353535;color:#fff}.meta{margin:0 0 2.5rem}.tags::before{content:"\2022";margin-left:1rem}code,pre{color:#353535;font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;border:1px solid #353535;font-size:small}.highlight [class^=language-]{color:#fff}code{padding:.1rem;border:none}pre{padding:.5rem;overflow-x:auto}pre code{border:none}img{max-width:100%;border:1px solid #353535}hr{background:#353535;height:1px;border:0}ul{list-style-type:square}ul,ol{padding-left:1.2rem}header li,footer li{display:inline;text-transform:uppercase}header a,footer a{text-decoration:none}header ul,footer ul{justify-content:space-between;display:flex}[aria-current=page]{text-decoration:line-through}header,section,footer{padding:1rem 0}blockquote{border-left:5px solid #353535;padding-left:1rem}.posts ul,header ul,footer ul{list-style:none}.posts,header ul,footer ul{padding:0}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.7rem}.posts li a,.posts li div{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}.hash{opacity:.25;text-decoration:none}table{border-collapse:collapse;text-align:left;width:100%}table tr{background:#fff;border-bottom:1px solid}table th,table td{padding:10px 20px}</style><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"post","name":"","headline":"","alternativeHeadline":"","description":"Name: Vinay Kumar GitHub: https:\/\/github.com\/imflash217\/\nNotable Repositories with original implementation of AI systems:\n1. Cybertron # GitHub link: https:\/\/github.com\/imflash217\/cybertron\/blob\/main\/cybertron\/\nSummary: It implements various Transformer based algorithms from scratch in raw-python, for auto-regressive tasks, such as:\nvarious embedding layers like: Rotary_Embedding, Absolute_Positional, Fixed_Positional, Relative_PositionBias, Alibi_PositionBias Residual \u0026amp; GRU gates various Normalization layers like ReZero, ScaleNorm, RMSNorm etc. A unified attention layer class to support various configurations of transformer-based architectures. Vision Transformer (ViT) Wrapper Native implementation of Encoder \u0026amp; Decoder blocks.","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/localhost:1313\/post\/github_repos\/"},"author":{"@type":"Person","name":""},"creator":{"@type":"Person","name":""},"accountablePerson":{"@type":"Person","name":""},"copyrightHolder":"Vinay Kumar","copyrightYear":"0001","dateCreated":"0001-01-01T00:00:00.00Z","datePublished":"0001-01-01T00:00:00.00Z","dateModified":"0001-01-01T00:00:00.00Z","publisher":{"@type":"Organization","name":"Vinay Kumar","url":"http://localhost:1313/","logo":{"@type":"ImageObject","url":"http:\/\/localhost:1313\/","width":"32","height":"32"}},"image":"http://localhost:1313/","url":"http:\/\/localhost:1313\/post\/github_repos\/","wordCount":"188","genre":[],"keywords":[]}</script></head><body><main><header><nav><ul><li><a href=/>Index</a></li><li><a href=/about/>About</a></li><li><a href=/atom.xml>RSS</a></li></ul></nav></header><hr><section><span itemprop=articleBody><p>Name: Vinay Kumar
GitHub: <a href=https://github.com/imflash217/>https://github.com/imflash217/</a></p><hr><p><code>Notable Repositories</code> with original implementation of AI systems:</p><h3 id=1-cybertron>1. Cybertron <a href=#1-cybertron class=hash>#</a></h3><ul><li><p><strong>GitHub link</strong>: <a href=https://github.com/imflash217/cybertron/blob/main/cybertron/>https://github.com/imflash217/cybertron/blob/main/cybertron/</a></p></li><li><p><strong>Summary</strong>: It implements various Transformer based algorithms from scratch in raw-python, for auto-regressive tasks, such as:</p><ul><li>various embedding layers like: <code>Rotary_Embedding</code>, <code>Absolute_Positional</code>, <code>Fixed_Positional</code>, <code>Relative_PositionBias</code>, <code>Alibi_PositionBias</code></li><li>Residual & GRU gates</li><li>various Normalization layers like <code>ReZero</code>, <code>ScaleNorm</code>, <code>RMSNorm</code> etc.</li><li>A unified attention layer class to support various configurations of transformer-based architectures.</li><li>Vision Transformer (ViT) Wrapper</li><li>Native implementation of Encoder & Decoder blocks.</li><li>Various utility methods like <code>top_k</code>, <code>top_p</code>, <code>top_a</code> etc to support the modular composition of various architectural components.</li></ul></li><li><p><strong>Navigation guide</strong>: To navigate the repository, look at the <code>cybertron</code> folder. All implementation of various algorithms can be found inside <code>transformers.py</code> file and the autoregressive wrapper is written in <code>autoregressive_wrapper.py</code> file.</p></li></ul><h3 id=2-flashgrid>2. FlashGrid <a href=#2-flashgrid class=hash>#</a></h3><ul><li><strong>GitHub link</strong>: <a href=https://github.com/imflash217/flashGrid/tree/main>https://github.com/imflash217/flashGrid/tree/main</a></li><li><strong>Summary & Navigation</strong>: PyTorch implementation of GridCells and PlaceCells for Object Detection Task. This was my Master&rsquo;s Thesis Project.<ul><li>It implements various algorithms for learning grid-cells pattern with different grid-initialization strategies. It implements MotionMatrix and various loss functions in learning grid-cells like patterns in <code>src/model.py</code>.</li><li>Generating proper data for training GridCells is written in <code>src/data_io.py</code></li></ul></li></ul></span></section><hr><footer><nav><ul><li>Â© 2025</li></ul></nav></footer></main></body></html>